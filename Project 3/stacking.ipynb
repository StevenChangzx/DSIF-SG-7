{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn. compose import ColumnTransformer \n",
    "from sklearn.metrics import *\n",
    "from sklearn.ensemble import VotingClassifier, AdaBoostClassifier,GradientBoostingClassifier, StackingClassifier\n",
    "import xgboost as xgb\n",
    "import stacking as sk\n",
    "\n",
    "RANDOM_STATE : int = 42\n",
    "TARGET_NAME : str = \"target\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### candidates models\n",
    "| Vectorizer |stop_words | ngram_range  |  min_df | max_features  |  max_df | Model| penalty | C \n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "| CountVectorizer |english |  (1, 2) | 2  |  3500 | 0.5  |  RandomForestClassifier(max_depth=20, n_estimators=100)| NaN|NaN |\n",
    "| TfidfVectorizer/CountVectorizer |english |  (1, 1) | 2  |  3500 | 0.7  | RandomForestClassifier(max_depth=20, n_estimators=50)| NaN|NaN |\n",
    "| TfidfVectorizer |english |  (1, 1)|  1 |  2000 |  0.7 | LogisticRegression(C=0.1, solver='saga')|\tL2|  0.1|\n",
    "| TfidfVectorizer/CountVectorizer |english |  (1, 2)|  2 |  3000 |  0.7 | GradientBoostingClassifier(max_depth=20, n_estimators=250)|NaN|NaN |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the CountVectorizer for Classifiers (Estimators)\n",
    "text_preprocessing_a = Pipeline([('Vect', CountVectorizer(stop_words='english', max_features=3500, min_df=2, max_df=0.5, ngram_range=(1,2)))])\n",
    "text_preprocessing_b = Pipeline([('Vect', CountVectorizer(stop_words='english',max_features=3500, min_df=2, max_df=0.7, ngram_range=(1,1)))])\n",
    "text_preprocessing_c = Pipeline([('Vect', CountVectorizer(stop_words='english',max_features=2000, min_df=1, max_df=0.7, ngram_range=(1,1)))])\n",
    "text_preprocessing_d = Pipeline([('Vect', CountVectorizer(stop_words='english',max_features=3000, min_df=2, max_df=0.7, ngram_range=(1,2)))])\n",
    "\n",
    "# Setting up the TfidfVectorizer for Classifiers (Estimators)\n",
    "text_preprocessing_2 = Pipeline([('Vect', TfidfVectorizer(stop_words='english',max_features=3500, min_df=2, max_df=0.7, ngram_range=(1,1)))])\n",
    "text_preprocessing_3 = Pipeline([('Vect', TfidfVectorizer(stop_words='english',max_features=2000, min_df=1, max_df=0.7, ngram_range=(1,1)))])\n",
    "text_preprocessing_4 = Pipeline([('Vect', TfidfVectorizer(stop_words='english',max_features=3000, min_df=2, max_df=0.7, ngram_range=(1,2)))])\n",
    "\n",
    "def pre(proc):\n",
    "    return ColumnTransformer(\n",
    "                            [('text_preprocessing', proc, 'lem_text')]\n",
    "                            )\n",
    "\n",
    "# Moodels with CountVectorizer                      \n",
    "Model_C1 = Pipeline([\n",
    "    ('pre', pre( text_preprocessing_a )),\n",
    "    ('classifier', RandomForestClassifier(max_depth=20, n_estimators=100))\n",
    "])\n",
    "\n",
    "Model_C2 = Pipeline([\n",
    "    ('pre', pre( text_preprocessing_b )),\n",
    "    ('classifier', RandomForestClassifier(max_depth=20, n_estimators=50))\n",
    "])\n",
    "\n",
    "Model_C3 = Pipeline([\n",
    "    ('pre', pre( text_preprocessing_d )),\n",
    "    ('classifier', GradientBoostingClassifier(max_depth=20, n_estimators=250))\n",
    "])\n",
    "\n",
    "# Moodels with TfidfVectorizer\n",
    "Model_T1 = Pipeline([\n",
    "    ('pre', pre( text_preprocessing_2 )),\n",
    "    ('classifier', RandomForestClassifier(max_depth=20, n_estimators=50))\n",
    "])\n",
    "\n",
    "Model_T2 = Pipeline([\n",
    "    ('pre', pre( text_preprocessing_3 )),\n",
    "    ('classifier', LogisticRegression(C=0.1, solver='saga'))\n",
    "])\n",
    "\n",
    "Model_T3 = Pipeline([\n",
    "    ('pre', pre( text_preprocessing_4 )),\n",
    "    ('classifier', GradientBoostingClassifier(max_depth=20, n_estimators=250))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_nlp.csv')\n",
    "df = df[['lem_text','y']]\n",
    "y = df['y']\n",
    "X = df.drop('y', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model 3: Stacking based on candidates models\n",
    "\n",
    "![](output/Screenshot%202022-11-24%20at%201.25.04%20AM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'hear forever stamp thinking secure investment utility appreciate backed reputable institution hit forever stamp figured degens would fun poking hole thesis forever stamp released may stamp forever stamp today may stamp year stamp appreciation mail letter mean come may attractive investment yield great return really resell marketplace blackmarket hot dog grandma interested dumping net worth forever stamp ampx bwhat thought'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/zhangzhexu/Desktop/proj 3/esm Model.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangzhexu/Desktop/proj%203/esm%20Model.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m stacking_model \u001b[39m=\u001b[39m StackingClassifier(estimators\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(level_1_classifiers\u001b[39m.\u001b[39mitems()), final_estimator\u001b[39m=\u001b[39mlevel_2_classifier, passthrough\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, cv\u001b[39m=\u001b[39mkfold, stack_method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpredict_proba\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangzhexu/Desktop/proj%203/esm%20Model.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m level_1_columns \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m_prediction\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m level_1_classifiers\u001b[39m.\u001b[39mkeys()]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhangzhexu/Desktop/proj%203/esm%20Model.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m pd\u001b[39m.\u001b[39mDataFrame(stacking_model\u001b[39m.\u001b[39;49mfit_transform(X_train, y_train), columns\u001b[39m=\u001b[39mlevel_1_columns \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(X_train\u001b[39m.\u001b[39mcolumns))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:870\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    868\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m--> 870\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_stacking.py:584\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_le \u001b[39m=\u001b[39m LabelEncoder()\u001b[39m.\u001b[39mfit(y)\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_le\u001b[39m.\u001b[39mclasses_\n\u001b[0;32m--> 584\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_le\u001b[39m.\u001b[39;49mtransform(y), sample_weight)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_stacking.py:256\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_method_ \u001b[39m=\u001b[39m [\n\u001b[1;32m    250\u001b[0m     meth\n\u001b[1;32m    251\u001b[0m     \u001b[39mfor\u001b[39;00m (meth, est) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_method_, all_estimators)\n\u001b[1;32m    252\u001b[0m     \u001b[39mif\u001b[39;00m est \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdrop\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m ]\n\u001b[1;32m    255\u001b[0m X_meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concatenate_predictions(X, predictions)\n\u001b[0;32m--> 256\u001b[0m _fit_single_estimator(\n\u001b[1;32m    257\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfinal_estimator_, X_meta, y, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    260\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_base.py:47\u001b[0m, in \u001b[0;36m_fit_single_estimator\u001b[0;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m---> 47\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[1;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m estimator\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1138\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     _dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32]\n\u001b[0;32m-> 1138\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1139\u001b[0m     X,\n\u001b[1;32m   1140\u001b[0m     y,\n\u001b[1;32m   1141\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1142\u001b[0m     dtype\u001b[39m=\u001b[39;49m_dtype,\n\u001b[1;32m   1143\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1144\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49msolver \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m [\u001b[39m\"\u001b[39;49m\u001b[39mliblinear\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msag\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1145\u001b[0m )\n\u001b[1;32m   1146\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    595\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1074\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1070\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1071\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[0;32m-> 1074\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1075\u001b[0m     X,\n\u001b[1;32m   1076\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1077\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1078\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1079\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1080\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1081\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1082\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1083\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1084\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1085\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1086\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1087\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1088\u001b[0m )\n\u001b[1;32m   1090\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1092\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    854\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    855\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    857\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    858\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    860\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'hear forever stamp thinking secure investment utility appreciate backed reputable institution hit forever stamp figured degens would fun poking hole thesis forever stamp released may stamp forever stamp today may stamp year stamp appreciation mail letter mean come may attractive investment yield great return really resell marketplace blackmarket hot dog grandma interested dumping net worth forever stamp ampx bwhat thought'"
     ]
    }
   ],
   "source": [
    "level_1_classifiers = dict()\n",
    "level_1_classifiers[\"C1\"] = Model_C1\n",
    "level_1_classifiers[\"C2\"] = Model_C2\n",
    "level_1_classifiers[\"C3\"] = Model_C3\n",
    "level_1_classifiers[\"T1\"] = Model_T1\n",
    "level_1_classifiers[\"T2\"] = Model_T2\n",
    "level_1_classifiers[\"T3\"] = Model_T3\n",
    "\n",
    "# meta classifier\n",
    "level_2_classifier = LogisticRegression(random_state=RANDOM_STATE)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "stacking_model = StackingClassifier(estimators=list(level_1_classifiers.items()), final_estimator=level_2_classifier, passthrough=True, cv=kfold, stack_method=\"predict_proba\")\n",
    "\n",
    "level_1_columns = [f\"{name}_prediction\" for name in level_1_classifiers.keys()]\n",
    "pd.DataFrame(stacking_model.fit_transform(X_train, y_train), columns=level_1_columns + list(X_train.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred0 = stacking_model.predict(X_test)\n",
    "y_test_pred0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "level_1 = sk.Level1Stacker(copy.deepcopy(level_1_classifiers), passthrough=True, save_x=True)\n",
    "level_2 = sk.Level2Stacker(LogisticRegression(random_state=RANDOM_STATE))\n",
    "\n",
    "final_stacking_model = Pipeline([\n",
    "                            ('level_1', level_1), \n",
    "                            ('level_2', level_2) \n",
    "                            ])\n",
    "\n",
    "final_stacking_model.fit(X_train, y_train)\n",
    "#level_1.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = final_stacking_model.predict(X_test)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy of scikit-learn stacking classifier: {accuracy_score(y_test, y_test_pred)}\")\n",
    "\n",
    "for name, classifier in level_1_classifiers.items():\n",
    "    classifier_ = copy.deepcopy(classifier)\n",
    "    classifier_.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Accuracy of standalone {name} classifier: {accuracy_score(y_test, classifier_.predict(X_test))}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "413e33381bdc50bcb6ec31cde5fd333ab8a35bce4ebe7073ac38c207a09a349f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
